from __future__ import annotations

import math
import warnings
import torch
import torch.nn as nn
from seizure_pred.training.registries import MODELS

# Helper functions (Truncated Normal and DropPath)
def _trunc_normal_(tensor, mean, std, a, b):
    # Function to sample from a truncated normal distribution
    def norm_cdf(x):
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Values are generated by transforming a uniform variable from [2l-1, 2u-1]
    # to the normal distribution using the inverse CDF (erfinv).
    tensor.uniform_(2 * l - 1, 2 * u - 1)
    tensor.erfinv_()

    # Transform to proper normal distribution
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure values are within [a, b]
    tensor.clamp_(min=a, max=b)
    return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # Wrapper function for truncated normal initialization with no_grad context
    with torch.no_grad():
        return _trunc_normal_(tensor, mean, std, a, b)

def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):
    # Implements DropPath (stochastic depth) regularization
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    # Shape for broadcasting the random tensor (batch_size, 1, 1, ...)
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)
    # Create a random binary tensor (0 or 1) based on keep_prob
    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)
    if keep_prob > 0.0 and scale_by_keep:
        # Scale the tensor by 1/keep_prob during training to maintain expected value
        random_tensor.div_(keep_prob)
    return x * random_tensor

class DropPath(nn.Module):
    # DropPath module wrapper
    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob
        self.scale_by_keep = scale_by_keep

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)

# Model Components
class ICB(nn.Module):
    # Inception Conv Block (ICB)
    def __init__(self, in_features, hidden_features, drop=0.):
        super().__init__()
        # Parallel 1x1 and 3x3 convolutions
        self.conv1 = nn.Conv1d(in_features, hidden_features, 1)
        self.conv2 = nn.Conv1d(in_features, hidden_features, 3, 1, 1) # padding=1 keeps length same
        # Output convolution
        self.conv3 = nn.Conv1d(hidden_features, in_features, 1)
        self.drop = nn.Dropout(drop)
        self.act = nn.GELU()

    def forward(self, x):
        # Input shape expected: (Batch, Sequence, Features)
        # Transpose for Conv1d: (Batch, Features, Sequence)
        x = x.transpose(1, 2)

        x1 = self.conv1(x)
        x1_1 = self.act(x1)
        x1_2 = self.drop(x1_1)

        x2 = self.conv2(x)
        x2_1 = self.act(x2)
        x2_2 = self.drop(x2_1)

        # Gating mechanism
        out1 = x1 * x2_2
        out2 = x2 * x1_2

        x = self.conv3(out1 + out2)
        # Transpose back: (Batch, Sequence, Features)
        x = x.transpose(1, 2)
        return x

class PatchEmbed(nn.Module):
    # Patch Embedding using Conv1d with stride
    def __init__(self, seq_len, patch_size=8, in_chans=3, embed_dim=384):
        super().__init__()
        stride = patch_size // 2 # Overlapping patches
        # Calculate number of patches based on sequence length, kernel size, and stride
        num_patches = int((seq_len - patch_size) / stride + 1)
        self.num_patches = num_patches
        # Conv1d acts as the patch embedding layer
        self.proj = nn.Conv1d(in_chans, embed_dim,
                              kernel_size=patch_size, stride=stride)

    def forward(self, x):
        # Input shape: (Batch, Channels, SequenceLength)
        # Output: (Batch, EmbedDim, NumPatches) -> Flatten -> Transpose
        # Output shape: (Batch, NumPatches, EmbedDim)
        x_out = self.proj(x).flatten(2).transpose(1, 2)
        return x_out

class AdaptiveSpectralBlock(nn.Module):
    # Adaptive Spectral Block (ASB) using FFT
    def __init__(self, dim, adaptive_filter=True):
        super().__init__()
        self.adaptive_filter = adaptive_filter

        # Learnable complex weights (represented as real pairs) for filtering
        self.complex_weight_high = nn.Parameter(
            torch.randn(dim, 2, dtype=torch.float32) * 0.02)
        self.complex_weight = nn.Parameter(
            torch.randn(dim, 2, dtype=torch.float32) * 0.02)

        # Initialize weights using truncated normal distribution
        trunc_normal_(self.complex_weight_high, std=.02)
        trunc_normal_(self.complex_weight, std=.02)
        # Learnable threshold parameter for adaptive masking
        self.threshold_param = nn.Parameter(torch.rand(1))

    def create_adaptive_high_freq_mask(self, x_fft):
        B, N, C = x_fft.shape # Batch, FreqBins, EmbedDim

        # Calculate energy (magnitude squared)
        energy = torch.abs(x_fft).pow(2).sum(dim=-1) # Sum over EmbedDim? Or should it be abs()? Check original paper/code. Assuming sum over EmbedDim based on code structure.

        # Normalize energy based on median energy within the batch
        flat_energy = energy.view(B, -1)
        median_energy = flat_energy.median(dim=1, keepdim=True)[0]
        median_energy = median_energy.view(B, 1)

        epsilon = 1e-6 # Avoid division by zero
        normalized_energy = energy / (median_energy + epsilon)

        # Create adaptive mask based on threshold (using STE for differentiability)
        adaptive_mask = ((normalized_energy > self.threshold_param).float() - self.threshold_param).detach() + self.threshold_param
        adaptive_mask = adaptive_mask.unsqueeze(-1) # Add dim for broadcasting

        return adaptive_mask

    def forward(self, x_in):
        B, N, C = x_in.shape # Batch, NumPatches, EmbedDim

        dtype = x_in.dtype
        x = x_in.to(torch.float32)

        # FFT along the patch sequence dimension
        x_fft = torch.fft.rfft(x, dim=1, norm='ortho') # (B, N//2+1, C)

        # Apply base complex filter weights
        weight = torch.view_as_complex(self.complex_weight)
        x_weighted = x_fft * weight

        # Apply adaptive high-frequency filter if enabled
        if self.adaptive_filter:
            freq_mask = self.create_adaptive_high_freq_mask(x_fft)
            x_masked = x_fft * freq_mask.to(x.device)

            weight_high = torch.view_as_complex(self.complex_weight_high)
            x_weighted2 = x_masked * weight_high

            x_weighted = x_weighted + x_weighted2 # Combine filters

        # Inverse FFT to go back to time domain
        x = torch.fft.irfft(x_weighted, n=N, dim=1, norm='ortho')

        x = x.to(dtype)
        x = x.view(B, N, C) # Ensure correct shape

        return x

class TSLANetLayer(nn.Module):
    # Single TSLANet Layer combining ASB and ICB with residual connections
    def __init__(self, dim, mlp_ratio=3., drop=0., drop_path=0., norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.ASB = AdaptiveSpectralBlock(dim)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.ICB = ICB(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)

    def forward(self, x):
        # Apply ASB -> Norm -> ICB within a residual connection
        x = x + self.drop_path(self.ICB(self.norm2(self.ASB(self.norm1(x)))))
        return x

# Main TSLANet Model
class TSLANet(nn.Module):
    """
    A time series lightweight adaptive network for EEG classification.
    Expects input shape: (batch_size, num_electrodes, chunk_size)
    """
    def __init__(self,
                 chunk_size: int = 640,    # Adjusted default
                 patch_size: int = 32,     # Adjusted default
                 num_electrodes: int = 18, # Adjusted default
                 emb_dim: int = 128,       # Default from torcheeg
                 dropout_rate: float = 0.15,# Default from torcheeg
                 depth: int = 2,           # Default from torcheeg
                 num_classes: int = 2):    # Adjusted default
        super().__init__()
        self.emb_dim = emb_dim

        # Patch embedding layer
        self.patch_embed = PatchEmbed(
            seq_len=chunk_size, patch_size=patch_size,
            in_chans=num_electrodes, embed_dim=emb_dim
        )
        num_patches = self.patch_embed.num_patches

        # Positional embedding (learnable)
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, emb_dim), requires_grad=True)
        self.pos_drop = nn.Dropout(p=dropout_rate)

        # Optional: Linear layer (removed as PatchEmbed handles this)
        # self.input_layer = nn.Linear(patch_size, emb_dim)

        # Stochastic depth dropout probabilities
        dpr = [x.item() for x in torch.linspace(0, dropout_rate, depth)]

        # TSLANet blocks
        self.tsla_blocks = nn.ModuleList([
            TSLANetLayer(dim=emb_dim, drop=dropout_rate, drop_path=dpr[i])
            for i in range(depth)]
        )

        # Classification head
        self.head = nn.Linear(emb_dim, num_classes)

        # Initialize weights
        trunc_normal_(self.pos_embed, std=.02)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        # Weight initialization for Linear and LayerNorm layers
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Input x: (batch_size, num_electrodes, chunk_size)
        """
        # 1. Patch Embedding
        # Output: (batch_size, num_patches, emb_dim)
        x = self.patch_embed(x)

        # 2. Add Positional Embedding
        x = x + self.pos_embed
        x = self.pos_drop(x)

        # 3. Pass through TSLANet Layers
        for tsla_blk in self.tsla_blocks:
            x = tsla_blk(x)

        # 4. Global Average Pooling (across patches)
        # Input: (batch_size, num_patches, emb_dim)
        # Output: (batch_size, emb_dim)
        x = x.mean(dim=1) # Average pool over the sequence/patch dimension

        # 5. Classification Head
        # Output: (batch_size, num_classes)
        x = self.head(x)
        return x

from seizure_pred.core.config import ModelConfig

@MODELS.register("tslanet", help="TSLANet baseline model.")
def build_tslanet(cfg: ModelConfig):
    kw = dict(getattr(cfg, "kwargs", {}) or {})
    in_ch = cfg.in_channels or kw.get("in_channels", kw.get("num_electrodes", 19))
    seq_len = kw.get("chunk_size", kw.get("seq_len", 256))
    return TSLANet(
        in_channels=int(in_ch),
        seq_len=int(seq_len),
        num_classes=int(getattr(cfg, "num_classes", 2)),
        **{k:v for k,v in kw.items() if k not in {"in_channels","num_electrodes","chunk_size","seq_len"}}
    )
