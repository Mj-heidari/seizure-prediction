import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math


from importlib import resources as _resources

def _load_packaged_ch_locs() -> "np.ndarray | None":
    """Best-effort load of packaged ch_locs.npy (shape: [n_ch, 2])."""
    try:
        res = _resources.files("seizure_pred.models").joinpath("ch_locs.npy")
        with _resources.as_file(res) as p:
            return np.load(str(p))
    except Exception:
        return None
from seizure_pred.core.config import ModelConfig
from seizure_pred.training.registries import MODELS

class MultiScaleTemporalConv(nn.Module):
    """
    Multi-Scale Temporal-Conv Branch (MBdMGC-CWTFFNet)
    --------------------------------------------------
    Inputs:
        x: Tensor of shape (B, C, S)  # B=batch, C=channels, S=time samples

    Behavior:
        - Applies n parallel depthwise temporal Conv1d blocks:
            Conv1d(in=C, out=C, groups=C, kernel_size[k], stride[k], padding)
            + BatchNorm1d(C) + ELU
        - Concatenates outputs along time dimension to form:
            FT: (B, C, sum_k T_k)

    Args:
        in_channels (int): number of EEG channels C
        kernel_sizes (list[int]): kernel sizes per branch (length n)
        strides (int | list[int], optional): stride(s) per branch. Default 1.
        paddings ('same' | int | list[int], optional): padding(s) per branch.
            - 'same' keeps time length when stride=1 (PyTorch≥1.10 supports padding='same').
            - Or provide int/list to control T_k explicitly.
        dilations (int | list[int], optional): dilation(s) per branch. Default 1.
        use_bias (bool): whether Conv1d uses bias. Default False (BN present).
    """
    def __init__(
        self,
        in_channels: int,
        kernel_sizes,
        strides=1,
        paddings='same',
        dilations=1,
        use_bias=False,
    ):
        super().__init__()
        if isinstance(kernel_sizes, int):
            kernel_sizes = [kernel_sizes]
        n = len(kernel_sizes)

        # Normalize list-like hyperparams to per-branch lists
        def to_list(x):
            return x if isinstance(x, (list, tuple)) else [x] * n

        strides   = to_list(strides)
        dilations = to_list(dilations)
        paddings  = paddings if isinstance(paddings, (list, tuple)) else [paddings] * n

        self.in_channels = in_channels
        self.n_branches = n
        self.branches = nn.ModuleList()

        for k in range(n):
            conv = nn.Conv1d(
                in_channels=in_channels,
                out_channels=in_channels,   # depthwise -> preserve C
                kernel_size=kernel_sizes[k],
                stride=strides[k],
                padding=paddings[k],
                dilation=dilations[k],
                groups=in_channels,         # depthwise over channels
                bias=use_bias,
            )
            bn = nn.BatchNorm1d(in_channels)
            act = nn.ELU(inplace=True)
            self.branches.append(nn.Sequential(conv, bn, act))

        self._init_weights()

    def _init_weights(self):
        # Kaiming init for ELU
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (B, C, S)
        returns FT: (B, C, sum_k T_k)  # concatenated along time
        """
        outs = [branch(x) for branch in self.branches]
        # Concatenate along time dimension (dim=2)
        FT = torch.cat(outs, dim=2)
        return FT

class MultiBandSpectralConv(nn.Module):
    """
    Multi-Band Spectral-Conv Branch using fixed Db4 WaveConv operators.

    - Performs L-level DWT (L = floor(log2(fs)) - 3, min 1).
    - At each level: convolution with lowpass (h) and highpass (g) Db4 filters
      (filters are fixed / non-learnable), then decimation by 2.
    - Gathers detail coefficients at each level and the final approximation.
    - Maps levels to physiological bands (delta, theta, alpha, beta, gamma)
      by finding the decomposition level whose frequency range overlaps the band.
    - Returns F_delta..F_gamma (each shape (B, C, H_l)) and FR concatenated along time dim.
    """

    def __init__(self, fs: int):
        super().__init__()
        self.fs = fs
        L = max(1, math.floor(math.log2(fs)) - 3)
        self.levels = L

        # db4 scaling (lowpass) coefficients (orthonormal)
        # (values commonly used for Daubechies 4)
        h = [
            -0.010597401785069032,
             0.032883011666982945,
             0.030841381835560763,
            -0.18703481171888114,
            -0.02798376941698385,
             0.6308807679295904,
             0.7148465705529154,
             0.2303778133088964
        ]
        h = torch.tensor(h, dtype=torch.float32)  # lowpass
        # highpass filter for decomposition: g[k] = (-1)^{k} * h[::-1][k]
        g = torch.tensor([( (-1)**k ) * v for k, v in enumerate(h.flip(0))], dtype=torch.float32)

        # Save filters as buffers (non-learnable)
        self.register_buffer('db4_low', h)   # shape (K,)
        self.register_buffer('db4_high', g)  # shape (K,)

    def _conv_groups(self, x, filt):
        # x: (B, C, S)
        # filt: 1D tensor (K,)
        B, C, S = x.shape
        K = filt.shape[0]
        # Prepare weight shape (C, 1, K) to perform grouped conv (depthwise per channel)
        w = filt.view(1, 1, K).repeat(C, 1, 1)  # (C,1,K)
        # pad signal to mimic symmetric extension (DWT usually uses signal extension)
        # we'll use reflection padding of size K-1 on the left to keep alignment
        pad = K - 1
        x_p = F.pad(x, (pad, 0), mode='reflect')  # pad left
        # grouped conv: in_channels=C, out_channels=C, groups=C
        out = F.conv1d(x_p, w, bias=None, stride=1, groups=C)
        return out  # shape (B, C, S + pad - (K-1)) => (B, C, S)

    def forward(self, x):
        """
        x: (B, C, S)
        returns:
            F_delta, F_theta, F_alpha, F_beta, F_gamma  (each (B, C, H_i))
            FR: (B, C, sum(H_i)) concatenated along time dim
        """
        # B, C, S = x.shape
        approx = x  # initial approximation coefficients
        details = []  # detail coefficients at each level 1..L
        approximations = []  # approximation after each level (we keep last too)

        # iterative DWT decomposition
        for _ in range(1, self.levels + 1):
            # lowpass conv then downsample (approximation)
            low = self._conv_groups(approx, self.db4_low)     # (B, C, S_current)
            high = self._conv_groups(approx, self.db4_high)   # (B, C, S_current)

            # downsample by 2 (decimation)
            low_ds = low[..., ::2]   # approximation for next level
            high_ds = high[..., ::2] # detail coefficients at this level

            details.append(high_ds)       # detail for level l : freq band (fs/2^{l+1}, fs/2^{l})
            approximations.append(low_ds) # store approximations too
            approx = low_ds                # feed into next iteration

        # Extract band tensors in order delta, theta, alpha, beta, gamma
        Fδ, Fθ, Fα, Fβ, Fγ = approx, details[-1], details[-2], details[-3], details[-4]
        FR = torch.cat([Fδ, Fθ, Fα, Fβ, Fγ], dim=2)

        return Fδ, Fθ, Fα, Fβ, Fγ, FR

class MultiChannelSpatialEncoding(nn.Module):
    """
    Multi-Channel Spatial-Encoding Branch
    -------------------------------------
    Inputs:
        x: (B, C, S)  # EEG trial (C channels, S samples)

    Components:
      1) Channel position encoder:
         - Builds adjacency matrix A ∈ R^{C×C} from electrode distances.
         - Formula:
             a_ij = 1/u_ij              if u_ij < mean(U)
                  = 0                   if u_ij ≥ mean(U)
                  = 1 / mean({u_ij<u})  if i == j
      2) Spatial feature encoder:
         - Channel-wise convolution across (C, S) plane.
         - Extracts multi-channel spatial features FS ∈ R^{B×C×DS}.
    """

    def __init__(self, channel_distances, out_channels=32, kernel_size=(18, 1)):
        """
        Args:
            channel_distances (torch.Tensor): (C, C) matrix of Euclidean distances
                between electrodes (from electrode positions).
            out_channels (int): output channels of spatial encoder.
            kernel_size (tuple): kernel size for Conv2d (channels × time).
        """
        super().__init__()
        assert channel_distances.ndim == 2 and channel_distances.shape[0] == channel_distances.shape[1]
        self.C = channel_distances.shape[0]

        # build adjacency matrix A ∈ R^{C×C}
        self.register_buffer("A", self._build_adjacency(channel_distances))

        # spatial feature encoder: Conv2d over (channels, time)
        # input reshaped to (B, 1, C, S)
        self.spatial_conv = nn.Sequential(
            nn.Conv2d(1, out_channels, kernel_size=kernel_size, padding="same"),
            nn.BatchNorm2d(out_channels),
            nn.ELU(inplace=True)
        )

    def _build_adjacency(self, dist: torch.Tensor):
        """
        Build adjacency matrix A from electrode distance matrix.
        """
        C = dist.shape[0]
        mask = torch.ones_like(dist, dtype=torch.bool)
        mask.fill_diagonal_(False)

        U = dist[mask]  # all u_ij, i != j
        mean_U = U.mean()

        A = torch.zeros_like(dist)

        for i in range(C):
            for j in range(C):
                if i == j:
                    # self-loop weight
                    valid_vals = U[U < mean_U]
                    if len(valid_vals) > 0:
                        A[i, j] = 1.0 / valid_vals.mean()
                    else:
                        A[i, j] = 1.0
                else:
                    if dist[i, j] < mean_U:
                        A[i, j] = 1.0 / dist[i, j]
                    else:
                        A[i, j] = 0.0
        return A

    def forward(self, x):
        """
        Args:
            x: (B, C, S)

        Returns:
            FS: (B, C, D_S)  # spatial features
            A:  (C, C) adjacency matrix
        """
        B, C, S = x.shape
        assert C == self.C, f"Expected {self.C} channels, got {C}"

        # Spatial feature encoding
        x_in = x.unsqueeze(1)          # (B, 1, C, S)
        feat = self.spatial_conv(x_in) # (B, out_ch, C, S)


        feat = feat.transpose(1, 2)        # (B, C, out_ch, S)
        FS = feat.flatten(2,3)        # (B, C, S*out_ch)

        return FS, self.A

class DynamicGraphConv(nn.Module):
    """
    Point-Wise Dynamic Multi-Graph Convolution Network (dMGCN)

    Args:
        C (int): number of EEG channels
        D (int): feature dimension of input map (DT, DS, or DR)
        reduction (int): reduction ratio 'r' for self-gating
    Inputs:
        x: (B, C, D)  feature map (FT, FS, or FR)
        A_init: (C, C) initial adjacency matrix from channel encoder
    Returns:
        G: (B, C, D)  dynamic graph features (GT, GS, or GR)
        A_dyn: (C, C) learned dynamic adjacency matrix
    """

    def __init__(self, C: int, D: int, reduction: int = 16):
        super().__init__()
        self.C = C
        self.D = D
        self.reduction = reduction

        # --- Self-gating for adjacency matrix ---
        self.fc1 = nn.Linear(C * C, (C * C) // reduction, bias=True)
        self.fc2 = nn.Linear((C * C) // reduction, C * C, bias=True)

        # --- Point-wise convolution kernels (two-layer MLP over feature dim) ---
        self.q1 = nn.Sequential(
            nn.Conv1d(in_channels=C, out_channels=C, kernel_size=1, bias=True),
            nn.LayerNorm(D),
            nn.ELU(),
        )
        self.q2 = nn.Sequential(
            nn.Conv1d(in_channels=C, out_channels=C, kernel_size=1, bias=True),
            nn.LayerNorm(D),
            nn.ELU(),
        )

        # init
        self._init_weights()

    def _init_weights(self):
        for m in [self.fc1, self.fc2, self.q1, self.q2]:
            for layer in m.modules():
                if isinstance(layer, (nn.Conv1d, nn.Linear)):
                    nn.init.xavier_uniform_(layer.weight)
                    if layer.bias is not None:
                        nn.init.zeros_(layer.bias)

    def forward(self, x: torch.Tensor, A_init: torch.Tensor):
        """
        Args:
            x: (B, C, D)
            A_init: (C, C)
        Returns:
            G: (B, C, D)
            A_dyn: (C, C)
        """
        B, C, D = x.shape
        assert C == self.C and D == self.D, f"Expected ({self.C},{self.D}), got ({C},{D})"

        # --- Step 1: Dynamic adjacency via self-gating ---
        A_vec = A_init.view(-1)                          # (C*C,)
        A_mid = F.elu(self.fc1(A_vec))                   # (C*C)//r
        A_dyn = F.relu(self.fc2(A_mid))                  # (C*C,)
        A_dyn = A_dyn.view(C, C)                         # reshape → (C,C)

        # Degree normalization: D^-1 A
        deg = A_dyn.sum(dim=1, keepdim=True) + 1e-6      # (C,1)
        A_norm = A_dyn / deg                             # row-normalized adjacency

        # --- Step 2: Point-wise graph convolution ---
        h = self.q1(x)              # (B, D, C)
        h = self.q2(h)              # (B, D, C)

        # message passing: (C,C) @ (B,C,D)
        # Use einsum: (C,C) * (B,C,D) -> (B,C,D)
        Axh = torch.einsum("ij,bjd->bid", A_norm, h)

        # residual + activation
        G = F.elu(Axh + x)               # (B,C,D)

        return G, A_dyn

class CWTFFNet(nn.Module):
    def __init__(self, in_dims, d_k=32, d_v=32, hidden_dim=32, dropout=0.1, num_classes=2):
        """
        Channel-Weighted Transformer Feature Fusion Network (CWTFFNet).

        Args:
            in_dims (list[int]): Input feature dims for [Temporal, Spatial, Spectral].
            d_k (int): Dimension for queries/keys.
            d_v (int): Dimension for values.
            hidden_dim (int): Hidden size for classifier.
            dropout (float): Dropout probability.
            num_classes (int): Number of output classes.
        """
        super(CWTFFNet, self).__init__()
        self.d_k = d_k
        self.d_v = d_v

        # Local CW-MHSA (3 heads)
        self.WQ = nn.ModuleList([nn.Linear(in_dim, d_k) for in_dim in in_dims])
        self.WK = nn.ModuleList([nn.Linear(in_dim, d_k) for in_dim in in_dims])
        self.WV = nn.ModuleList([nn.Linear(in_dim, d_v) for in_dim in in_dims])

        # Fully-Connected module
        self.fc = nn.Sequential(
            nn.LayerNorm(3 * d_v),
            nn.Linear(3 * d_v, 3 * d_v),
            nn.ReLU(),
            nn.Linear(3 * d_v, 3 * d_v),
        )

        # Feedforward module FM
        self.fm = nn.Sequential(
            nn.Linear(3 * d_v, 3 * d_v),
            nn.ELU(),
            nn.Linear(3 * d_v, 3 * d_v),
        )

        self.dropout = nn.Dropout(dropout)

        # Classifier (input is flattened later → set dynamically)
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        self.out_fc1 = nn.Linear(18 * 3 * d_v, self.hidden_dim)
        self.out_fc2 = nn.Linear(self.hidden_dim, self.num_classes)

    def forward(self, G_list, A_list, A_init):
        """
        Forward pass for CWTFFNet.

        Args:
            G_list (list[Tensor]): [GT, GS, GR], each ∈ (B, C, D).
            A_list (list[Tensor]): [AT, AS, AR], each ∈ (B, C, C).
            A_init (Tensor): Initialized adjacency matrix A ∈ (C, C).
        Returns:
            logits (Tensor): (B, num_classes).
            Z_fused (Tensor): (B, C, 3*d_v).
        """
        B, C, _ = G_list[0].shape
        Z_heads = []

        # 1) Local CW-MHSA
        for i in range(3):
            Q = self.WQ[i](G_list[i])  # (B, C, d_k)
            K = self.WK[i](G_list[i])  # (B, C, d_k)
            V = self.WV[i](G_list[i])  # (B, C, d_v)

            attn = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)  # (B, C, C)
            attn = F.softmax(attn, dim=-1)

            Z = torch.matmul(A_list[i], torch.matmul(attn, V))  # (B, C, d_v)
            Z_heads.append(Z)

        Z_local = torch.cat(Z_heads, dim=-1)  # (B, C, 3*d_v)

        # 2) Global CW-FFB
        A_vec = F.softmax(torch.mean(A_init, dim=-1), dim=0)[None,:,None] 
        channel_weights = F.softmax(Z_local.mean(dim=-1, keepdim=True), dim=1)
        Z_global = Z_local * (A_vec * channel_weights)

        # 3) Feedforward and Fully-Connected module
        Z_fused = self.fc(Z_global) + self.fm(Z_global)

        # 4) Classifier (simple MLP head)
        out = Z_fused.flatten(start_dim=1)  
        out = F.relu(self.out_fc1(out))
        logits = self.out_fc2(out)

        return logits, Z_fused

class MB_dMGC_CWTFFNet(nn.Module):
    def __init__(self, in_ch=18, sampling_rate = 128, d_model=32, d_k=32, d_v=32, coords = None):

      super(MB_dMGC_CWTFFNet, self).__init__()

      DT = 2560
      DS = 2560
      DR = 640

      self.mstc = MultiScaleTemporalConv(
          in_channels=in_ch,
          kernel_sizes=[5, 7, 11, 23],   # different temporal spans
          strides=1,                  # keep time length if paddings='same'
          paddings='same',            # requires PyTorch with padding='same' support
      )

      self.mbsc = MultiBandSpectralConv(fs=sampling_rate)

      # Coordinates for spatial encoding.
      # The original implementation loaded `ch_locs.npy` from the working directory.
      # For library use, we first try the packaged `seizure_pred.models/ch_locs.npy`.
      # If unavailable, we fall back to a simple deterministic circular layout.
      if coords is None:
          locs = _load_packaged_ch_locs()
          if (locs is not None and getattr(locs, 'ndim', 0) == 2 and locs.shape[1] >= 2 and locs.shape[0] >= in_ch):
              coords = torch.tensor(locs[:in_ch, :2], dtype=torch.float32)
          else:
              ang = torch.linspace(0, 2 * math.pi, steps=in_ch + 1)[:-1]
              coords = torch.stack([torch.cos(ang), torch.sin(ang)], dim=1)

      self.dist = torch.cdist(coords, coords, p=2)  # Euclidean distance matrix

      self.mcse = MultiChannelSpatialEncoding(channel_distances=self.dist, out_channels=4, kernel_size=(in_ch, 1))


      self.dgc_T = DynamicGraphConv(in_ch, DT, reduction=16)
      self.dgc_S = DynamicGraphConv(in_ch, DS, reduction=16)
      self.dgc_R = DynamicGraphConv(in_ch, DR, reduction=16)

      # CWTFFNet() 

      self.cwtff = CWTFFNet([DT, DS, DR])

    def forward(self,x):

      FT = self.mstc(x)
      Fd, Ft, Fa, Fb, Fg, FR = self.mbsc(x)

      FS, A = self.mcse(x)

      GT, AT = self.dgc_T(FT, A)
      GS, AS = self.dgc_S(FS, A)
      GR, AR = self.dgc_R(FR, A) 

      result = self.cwtff([GT, GS, GR], [AT, AS, AR], A)

      return result[0]


@MODELS.register("mb_dmgc_cwtffnet", help="MB-dMGC-CWTFFNet (multi-branch temporal + spatial + CWTFF).")
def build_mb_dmgc_cwtffnet(cfg: ModelConfig) -> nn.Module:
    """Factory wrapper.

    Notes:
      - This model expects input shaped as (B, C, T).
      - If you have known EEG channel coordinates, pass them via `cfg.kwargs['coords']`.
    """
    kw = dict(getattr(cfg, "kwargs", {}) or {})
    in_ch = int(getattr(cfg, "in_channels", kw.pop("in_ch", 18)) or 18)
    sampling_rate = int(kw.pop("sampling_rate", 128))
    return MB_dMGC_CWTFFNet(in_ch=in_ch, sampling_rate=sampling_rate, **kw)


if __name__ == "__main__":
    from torchinfo import summary

    torch.manual_seed(1)
    model = MB_dMGC_CWTFFNet().cuda()

    # Fake EEG input: (B, C, Bn, T)
    x = torch.randn(4, 18, 640).cuda()
    y = model(x)

    print("\nOutput shape:", y.shape)

    # Backward pass
    labels = torch.tensor([0, 1, 0, 1]).cuda()
    loss = F.cross_entropy(y, labels)
    loss.backward()

    summary(model, (2, 18, 640))
